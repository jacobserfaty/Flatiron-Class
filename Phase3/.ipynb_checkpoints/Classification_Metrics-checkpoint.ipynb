{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Classification Metrics\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC 2023\n",
    "<p>Phase </p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "- Calculate and interpret a confusion matrix\n",
    "- Calculate and interpret classification metrics such as accuracy, recall, and precision\n",
    "- Choose classification metrics appropriate to a business problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many classification metrics for evaluating model validation/test set performance:\n",
    "\n",
    "- Changes which model you will pick during hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Choice of evaluation metric:\n",
    "- Major impact on how well model serves its intended goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scenario: Identifying Fraudulent Credit Card Transactions\n",
    "<center><img src = \"Images/credit_card.png\" width = 400/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('data/credit_fraud_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Time    10000 non-null  float64\n",
      " 1   V1      10000 non-null  float64\n",
      " 2   V2      10000 non-null  float64\n",
      " 3   V3      10000 non-null  float64\n",
      " 4   V4      10000 non-null  float64\n",
      " 5   V5      10000 non-null  float64\n",
      " 6   V6      10000 non-null  float64\n",
      " 7   V7      10000 non-null  float64\n",
      " 8   V8      10000 non-null  float64\n",
      " 9   V9      10000 non-null  float64\n",
      " 10  V10     10000 non-null  float64\n",
      " 11  V11     10000 non-null  float64\n",
      " 12  V12     10000 non-null  float64\n",
      " 13  V13     10000 non-null  float64\n",
      " 14  V14     10000 non-null  float64\n",
      " 15  V15     10000 non-null  float64\n",
      " 16  V16     10000 non-null  float64\n",
      " 17  V17     10000 non-null  float64\n",
      " 18  V18     10000 non-null  float64\n",
      " 19  V19     10000 non-null  float64\n",
      " 20  V20     10000 non-null  float64\n",
      " 21  V21     10000 non-null  float64\n",
      " 22  V22     10000 non-null  float64\n",
      " 23  V23     10000 non-null  float64\n",
      " 24  V24     10000 non-null  float64\n",
      " 25  V25     10000 non-null  float64\n",
      " 26  V26     10000 non-null  float64\n",
      " 27  V27     10000 non-null  float64\n",
      " 28  V28     10000 non-null  float64\n",
      " 29  Amount  10000 non-null  float64\n",
      " 30  Class   10000 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "credit_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The dataset contains a bunch of features:\n",
    "- The transaction amount\n",
    "- The relative time of the transaction\n",
    "- V1-V28 are relevant features: product of feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fraud transaction algorithms:\n",
    "- Typically huge number of features \n",
    "- Can create small combination of features that encompass most variation in the full feature set:\n",
    "    - Principal component analysis (PCA)\n",
    "    - V1-V28 are these combination features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Target 'Class':\n",
    "- 1 if the transaction was fraudulent\n",
    "- 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9962\n",
       "1      38\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What have we just learned about our target in our dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run a logistic regression on the credit card fraud data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Separate data into feature and target DataFrames\n",
    "X = credit_data.drop('Class', axis = 1)\n",
    "y = credit_data['Class']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25,\n",
    "                                                   random_state=1)\n",
    "# Scale the data for modeling\n",
    "cred_scaler = StandardScaler()\n",
    "cred_scaler.fit(X_train)\n",
    "X_train_sc = cred_scaler.transform(X_train)\n",
    "X_test_sc = cred_scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regresssion model with the train data\n",
    "cred_model = LogisticRegression(random_state=42)\n",
    "cred_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember:\n",
    "- .score(X,y) gets the accuracy of our classification model on predicting y given X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cred_model.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We got 99.88% accuracy! \n",
    "- Our model is good. Right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Think again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Accuracy** = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "- Fraction of correct classifications.\n",
    "- What the `.score()` method calculates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Class 1 (Fraud) = Our positive class**\n",
    "\n",
    "- TP: True positive\n",
    "- FP: False positive\n",
    "- TN: True negative\n",
    "- FN: False negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='images/precisionrecall.png' width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Easy way to unpack the TP, TN, FP, FN is using the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "#nice function to visualize confusion matrix\n",
    "#from sklearn.metrics import plot_confusion_matrix #depreciated\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred = cred_model.predict(X_test_sc) \n",
    "# calculate confusion matrix\n",
    "cfmat = confusion_matrix(y_test, y_pred) \n",
    "\n",
    "cfmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice the way that sklearn displays its confusion matrix: The rows are \\['actually false', 'actually true'\\]; the columns are \\['predicted false', 'predicted true'\\].\n",
    "\n",
    "So it displays:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "TN & FP \\\\\n",
    "FN & TP\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cfmat.flatten()\n",
    "print(tn,fp,fn,tp)\n",
    "\n",
    "print(cfmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plot_confusion_matrix(cred_model,X_test_sc, y_test)\n",
    "#plt.show()\n",
    "ConfusionMatrixDisplay.from_estimator(cred_model, \n",
    "                      X_test_sc, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Accuracy** = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "**Precision:** Accuracy of positive and negative predictions.\n",
    "\n",
    "In words: How often did my model correctly identify transactions (fraudulent or not fraudulent)? This should give us the same value as we got from the `.score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cred_model.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "My accuracy is great. But is our model doing well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## True positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# true positives\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# false positives\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model not doing well on fraud detection.\n",
    "\n",
    "But the accuracy is great. What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Accuracy is not a great metric when:\n",
    "- There's a class imbalance\n",
    "- When we care about the positive detections rate for *each* given class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A better metric (for this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Precision** = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "**Precision:** Accuracy of positive predictions.\n",
    "\n",
    "In this case: \n",
    "- Of the model's prediction of 'fraudulent', how many of those predictions were correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prec = tp/(tp+fp)\n",
    "prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the given task of detecting credit card fraud:\n",
    "    \n",
    "Is precision something that the credit card company cares a lot about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Another metric that could be important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Recall** = **Sensitivity** = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "**Recall:** Fraction of positives that were correctly identified.\n",
    "\n",
    "\n",
    "\n",
    "Of the actual fraudulent transactions in our data, how many did our model predict as fraudulent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rec = tp / (tp + fn)\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this task, is recall an important metric? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A metric balancing both recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An $F$-score is a combination of precision and recall, which can be useful when both are important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $F_1$ score is an equal balance of the two using a [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean).\n",
    "\n",
    "$$F_1 = 2 \\frac{Precision \\cdot Recall}{Precision + Recall} = \\\\ \\frac{2TP}{2TP + FP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f1_sc = 2*prec*rec / (prec + rec)\n",
    "print(f1_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**F1 Score Interpretation**\n",
    "\n",
    "0.9\tVery good\n",
    "\n",
    "0.8 - 0.9 Good\n",
    "\n",
    "0.5 - 0.8 OK\n",
    "\n",
    "0.5\tNot good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which of these metrics do you think a credit card company would care most about when trying to flag fraudulent transactions to deny?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `classification_report()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The top rows show statistics for if you treated each label as the \"positive\" class\n",
    "- **Support** shows the sample size in each class\n",
    "- The averages in the bottom two rows are across the rows in the class table above (useful when there are more than two classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example: Breast Cancer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Load the data and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "cancer_data_dict = load_breast_cancer()\n",
    "X_cancer = cancer_data_dict['data']\n",
    "cancer_feature_names = cancer_data_dict['feature_names']\n",
    "\n",
    "cancer_features = pd.DataFrame(X_cancer, columns = cancer_feature_names)\n",
    "cancer_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_cancer = cancer_data_dict['target']\n",
    "cancer_data_dict['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cancer_data_dict['target']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - 0 = Malignant\n",
    " - 1 = Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(cancer_features, y_cancer,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Standard scale and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "bc_scaler = StandardScaler()\n",
    "bc_scaler.fit(X_train_bc)\n",
    "X_train_sc = bc_scaler.transform(X_train_bc)\n",
    "X_test_sc = bc_scaler.transform(X_test_bc)\n",
    "\n",
    "# Run the model\n",
    "bc_model = LogisticRegression(solver='lbfgs', max_iter=100, random_state=42)\n",
    "bc_model.fit(X_train_sc, y_train_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = bc_model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculate the following for this model:\n",
    "(scikit-learn's functions for this)\n",
    "\n",
    "- Confusion Matrix\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_bc, y_pred)\n",
    "\n",
    "#plot_confusion_matrix(bc_model, X_test_sc, y_test_bc);\n",
    "ConfusionMatrixDisplay.from_estimator(bc_model, X_test_sc, y_test_bc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(bc_model.score(X_test_sc, y_test_bc))\n",
    "print(precision_score(y_test_bc, y_pred))\n",
    "print(recall_score(y_test_bc, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_bc, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which of these metrics matter for this breast cancer detection problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Which metric to tune model hyperparameters with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy: misleading under class imbalance\n",
    "    - Sometimes just fine.\n",
    "- Precision: when false positives are much worse than false negatives\n",
    "    - DNA crime-scene forensics.\n",
    "- Recall: when false negatives are a lot worse \n",
    "    - X-ray imaging for cancer prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Multiclass classification**: more than two possible values for the target. An example:\n",
    "\n",
    "- Classifying iris sub-species based on petal/sepal characteristics.\n",
    "\n",
    "<center><img src = \"Images/iris-dataset.png\" width = 500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Same metrics/methods to evaluate our models:\n",
    "- Confusion matrices: number of rows/columns equal to the number of classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Metrics (precision/recall):\n",
    "    - choose one class to be the \"positive\" class.\n",
    "    - rest are assigned to the \"negative\" class. \n",
    "    - compute precision/recall for given \"positive\" class.\n",
    "\n",
    "Repeat for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_dict = load_iris()\n",
    "X = data_dict['data']\n",
    "features = pd.DataFrame(X, columns = data_dict['feature_names'])\n",
    "#print(flowers.DESCR)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = data_dict['target']\n",
    "data_dict['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0 = setosa\n",
    "- 1 = versicolor\n",
    "- 2 = virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train-test split \n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(features, y, test_size = 0.3, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Scale and transform\n",
    "iris_scaler = StandardScaler()\n",
    "X_train_iris_sc = iris_scaler.fit_transform(X_train_iris)\n",
    "X_test_iris_sc = iris_scaler.transform(X_test_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fit model and get predictions\n",
    "iris_model = LogisticRegression(max_iter = 10000)\n",
    "iris_model.fit(X_train_iris_sc, y_train_iris)\n",
    "y_pred_iris = iris_model.predict(X_test_iris_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our confusion matrix for the multiclass iris problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plot_confusion_matrix(iris_model, X_test_iris_sc,y_test_iris);\n",
    "ConfusionMatrixDisplay.from_estimator(iris_model, X_test_iris_sc, \n",
    "                      y_test_iris);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred_iris, y_test_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some issues with assessing the quality of a model solely on these metrics:\n",
    "- Need to think a little bit more carefully about probabilities and classification thresholds\n",
    "- The reciever operation curve (up next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary: Which Metric Should I Care About?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well, it depends.\n",
    "\n",
    "Accuracy:\n",
    "- Pro: Takes into account both false positives and false negatives.\n",
    "- Con: Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that *always* predicts a loser will be highly accurate.)\n",
    "\n",
    "Recall:\n",
    "- Pro: Highly sensitive to false negatives.\n",
    "- Con: No sensitivity to false positives.\n",
    "\n",
    "Precision:\n",
    "- Pro: Highly sensitive to false positives.\n",
    "- Con: No sensitivity to false negatives.\n",
    "\n",
    "F-1 Score:\n",
    "- Harmonic mean of recall and precision.\n",
    "\n",
    "The nature of your business problem will help you determine which metric matters.\n",
    "\n",
    "Sometimes false positives are much worse than false negatives: Arguably, a model that compares a sample of crime-scene DNA with the DNA in a city's database of its citizens presents one such case. Here a false positive would mean falsely identifying someone as having been present at a crime scene, whereas a false negative would mean only that we fail to identify someone who really was present at the crime scene as such.\n",
    "\n",
    "On the other hand, consider a model that inputs X-ray images and predicts the presence of cancer. Here false negatives are surely worse than false positives: A false positive means only that someone without cancer is misdiagnosed as having it, while a false negative means that someone with cancer is misdiagnosed as *not* having it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
